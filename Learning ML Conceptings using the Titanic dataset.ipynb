{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/commandante/getting-back-to-titanic-dataset?scriptVersionId=144194284\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Titanic - Machine Learning from Disaster\n\nHello, World! It's Kastav again.\n\nI'm currently pursuing the Data Science & ML course from ZTM and I've just completed the Scikit-Learn module.\nIt's an attempt to see if I have managed to learn anything at all. Hehe! (If I did, then I'm supposed to build a better performing model for the competition.)\n\n### Feel free to reach out to me over [LinkedIn](https://www.linkedin.com/in/commandantekaustav) so that we can learn, discuss and grow together. :)\n\nThe following image contains a standard workflow which will be followed throughout the project.\n![](https://raw.githubusercontent.com/commandantekaustav/Playing-with-the-Titanic-competition/main/ml101-6-step-ml-framework-tools-cropped.png)\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Problem Definition\n\n### Challenge Statement\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n\n\n**TL; DR;**\nPredict survival status (<span style=\"color:red\">Not</span> `Probability` as we already have the `label`) of a passenger by using their data.","metadata":{}},{"cell_type":"markdown","source":"## Data Desciption\n### Data Dictionary\n\n| Variable |\tDefinition |\tKey |\n|---|---|---|\n| survival |\tSurvival |\t0 = No, 1 = Yes |\n| pclass |\tTicket class |\t1 = 1st, 2 = 2nd, 3 = 3rd |\n| sex |\tSex |\t\n| Age |\tAge in years |\t\n| sibsp |\t# of siblings / spouses aboard the Titanic |\t\n| parch |\t# of parents / children aboard the Titanic |\t\n| ticket |\tTicket number |\t\n| fare |\tPassenger fare |\t\n| cabin |\tCabin number |\t\n| embarked |\tPort of Embarkation |\tC = Cherbourg, Q = Queenstown, S = Southampton |\n\n### Variable Notes\n- pclass: A proxy for socio-economic status (SES)\n    - 1st = Upper\n    - 2nd = Middle\n    - 3rd = Lower\n- age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n- sibsp: The dataset defines family relations in this way...\n- Sibling = brother, sister, stepbrother, stepsister\n- Spouse = husband, wife (mistresses and fiancés were ignored)\n- parch: The dataset defines family relations in this way...\n- Parent = mother, father\n- Child = daughter, son, stepdaughter, stepson\n- Some children travelled only with a nanny, therefore parch=0 for them.","metadata":{}},{"cell_type":"markdown","source":"## 0. Setting up The Environment","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nnp.random.seed(0)\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# pd.plotting.register_matplotlib_converters()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-25T15:13:36.891121Z","iopub.execute_input":"2023-09-25T15:13:36.891456Z","iopub.status.idle":"2023-09-25T15:13:37.537046Z","shell.execute_reply.started":"2023-09-25T15:13:36.891426Z","shell.execute_reply":"2023-09-25T15:13:37.535584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up aesthetics\nsns.set_style(style='ticks')\nsns.set(font_scale=1.2)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:37.538609Z","iopub.execute_input":"2023-09-25T15:13:37.53894Z","iopub.status.idle":"2023-09-25T15:13:37.545008Z","shell.execute_reply.started":"2023-09-25T15:13:37.53891Z","shell.execute_reply":"2023-09-25T15:13:37.544009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Data\n### Exploring the Nature of the data","metadata":{}},{"cell_type":"code","source":"# Importing data\ntitanic_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# Shape and Features\nshape = titanic_df.shape\nfeatures = titanic_df.columns\n\nprint(f\"Our data has the following shape {shape}\")\nprint(f\"Features present in our data are:\",features.to_list(),sep=\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:37.546016Z","iopub.execute_input":"2023-09-25T15:13:37.546313Z","iopub.status.idle":"2023-09-25T15:13:37.576274Z","shell.execute_reply.started":"2023-09-25T15:13:37.546287Z","shell.execute_reply":"2023-09-25T15:13:37.57502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:37.581419Z","iopub.execute_input":"2023-09-25T15:13:37.581871Z","iopub.status.idle":"2023-09-25T15:13:37.603109Z","shell.execute_reply.started":"2023-09-25T15:13:37.581839Z","shell.execute_reply":"2023-09-25T15:13:37.602174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:37.607061Z","iopub.execute_input":"2023-09-25T15:13:37.607424Z","iopub.status.idle":"2023-09-25T15:13:37.624067Z","shell.execute_reply.started":"2023-09-25T15:13:37.607365Z","shell.execute_reply":"2023-09-25T15:13:37.622911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:37.625688Z","iopub.execute_input":"2023-09-25T15:13:37.626129Z","iopub.status.idle":"2023-09-25T15:13:37.667341Z","shell.execute_reply.started":"2023-09-25T15:13:37.626087Z","shell.execute_reply":"2023-09-25T15:13:37.666154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring dtypes\ntitanic_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:37.668701Z","iopub.execute_input":"2023-09-25T15:13:37.669323Z","iopub.status.idle":"2023-09-25T15:13:37.677573Z","shell.execute_reply.started":"2023-09-25T15:13:37.669287Z","shell.execute_reply":"2023-09-25T15:13:37.676441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n## Numerical Data","metadata":{}},{"cell_type":"code","source":"# Performing some light data analysis\n\n# Features\nnumerical_features = ['Age', 'SibSp', 'Parch', 'Fare']\ncategorical_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket']\n\nprint('Numerical features:', numerical_features)\nprint('Categorical features:', categorical_features)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:37.679144Z","iopub.execute_input":"2023-09-25T15:13:37.679519Z","iopub.status.idle":"2023-09-25T15:13:37.688911Z","shell.execute_reply.started":"2023-09-25T15:13:37.679487Z","shell.execute_reply":"2023-09-25T15:13:37.688138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Numerical Data\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\nsns.histplot(data=titanic_df, x='Age', ax = axes[0,0]).set_title('Age')\nsns.histplot(data=titanic_df, x='SibSp', ax = axes[0,1]).set_title('SibSp')\nsns.histplot(data=titanic_df, x='Parch', ax = axes[1,0]).set_title('Parch')\nsns.histplot(data=titanic_df, x='Fare', ax = axes[1,1]).set_title('Fare')\nplt.tight_layout()\nplt.suptitle('Plotting Numerical Data to understand the distribution', y=1.02)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:37.689823Z","iopub.execute_input":"2023-09-25T15:13:37.690151Z","iopub.status.idle":"2023-09-25T15:13:39.594095Z","shell.execute_reply.started":"2023-09-25T15:13:37.690123Z","shell.execute_reply":"2023-09-25T15:13:39.593003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = titanic_df[numerical_features].corr()\nplt.figure(figsize=(6, 6))\nsns.heatmap(correlation_matrix, annot=True).set_title('Correlation among Numerical Features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:39.595261Z","iopub.execute_input":"2023-09-25T15:13:39.595848Z","iopub.status.idle":"2023-09-25T15:13:40.032342Z","shell.execute_reply.started":"2023-09-25T15:13:39.595813Z","shell.execute_reply":"2023-09-25T15:13:40.030815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing survival status over Numerical Features \npivot_table_df = pd.pivot_table(data = titanic_df, index = 'Survived', \n                                values = ['Age','SibSp','Parch','Fare'], \n                                aggfunc='mean').round(2) # Rounding of the cells by 2 digits\npivot_table_df\n\n# Nyah, heatmaps won't work much in this case. :D\n# plt.figure(figsize=(8, 2))\n# sns.heatmap(pivot_table_df, annot=True, fmt=\".2f\")\n# plt.title(\"Mean Values Heatmap - Titanic Dataset\")\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:40.033569Z","iopub.execute_input":"2023-09-25T15:13:40.033901Z","iopub.status.idle":"2023-09-25T15:13:40.058543Z","shell.execute_reply.started":"2023-09-25T15:13:40.033873Z","shell.execute_reply":"2023-09-25T15:13:40.057298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pairplotting of Numerical Data to fetch more insight\n\nplt.figure(figsize=(10, 10))  \nsns.pairplot(data=titanic_df, vars=numerical_features, kind='scatter', hue='Survived')\nplt.suptitle(\"Pair Plot of Titanic Data - Numerical Features\", y=1.02)\n# plt.tight_layout(pad=1.2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:40.060094Z","iopub.execute_input":"2023-09-25T15:13:40.06064Z","iopub.status.idle":"2023-09-25T15:13:47.808069Z","shell.execute_reply.started":"2023-09-25T15:13:40.060594Z","shell.execute_reply":"2023-09-25T15:13:47.806729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Data","metadata":{}},{"cell_type":"code","source":"# for feature in categorical_features:\n#     sns.barplot(x = titanic_df[feature].value_counts().index, \n#                 y = titanic_df[feature].value_counts()).set_title(feature)\n#     plt.show()\n    \nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n\n# Loop through the categorical features and plot bar plots in separate subplots\nfor i, feature in enumerate(categorical_features[:4]): # Not selecting Ticket\n    row = i // 2\n    col = i % 2\n    sns.barplot(x=titanic_df[feature].value_counts().index, y=titanic_df[feature].value_counts(), ax=axes[row, col])\n    axes[row, col].set_title(feature)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:47.815574Z","iopub.execute_input":"2023-09-25T15:13:47.816097Z","iopub.status.idle":"2023-09-25T15:13:51.54968Z","shell.execute_reply.started":"2023-09-25T15:13:47.816052Z","shell.execute_reply":"2023-09-25T15:13:51.548545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing survival and each of these categorical variables \nprint(pd.pivot_table(titanic_df, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(titanic_df, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(titanic_df, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:51.551554Z","iopub.execute_input":"2023-09-25T15:13:51.551964Z","iopub.status.idle":"2023-09-25T15:13:51.595809Z","shell.execute_reply.started":"2023-09-25T15:13:51.551931Z","shell.execute_reply":"2023-09-25T15:13:51.594553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing survival over Categorical Classes\n\nfig , axes = plt.subplots(1, 3, figsize=(12, 3))\nplt.suptitle(\"Comparing survival status over Categorical Classes\", y=1, fontsize=16)\n\nsns.heatmap(pd.pivot_table(titanic_df, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'), \n            ax = axes[0], cbar = False,\n            annot=True, fmt=\"\")\nsns.heatmap(pd.pivot_table(titanic_df, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'), \n            ax = axes[1], cbar = False,\n            annot=True, fmt=\"\")\nsns.heatmap(pd.pivot_table(titanic_df, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'), \n            ax = axes[2], cbar = False,\n            annot=True, fmt=\"\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:51.59705Z","iopub.execute_input":"2023-09-25T15:13:51.597373Z","iopub.status.idle":"2023-09-25T15:13:52.498891Z","shell.execute_reply.started":"2023-09-25T15:13:51.597344Z","shell.execute_reply":"2023-09-25T15:13:52.497587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detecting Missing Valus","metadata":{}},{"cell_type":"code","source":"titanic_df.isna().sum().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:52.500557Z","iopub.execute_input":"2023-09-25T15:13:52.50105Z","iopub.status.idle":"2023-09-25T15:13:52.513628Z","shell.execute_reply.started":"2023-09-25T15:13:52.500995Z","shell.execute_reply":"2023-09-25T15:13:52.512373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.heatmap(titanic_df.isna().transpose() ,cbar = False, cmap = 'winter').set_title('Missing Data in our Dataframe')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:52.514948Z","iopub.execute_input":"2023-09-25T15:13:52.515749Z","iopub.status.idle":"2023-09-25T15:13:53.724001Z","shell.execute_reply.started":"2023-09-25T15:13:52.515715Z","shell.execute_reply":"2023-09-25T15:13:53.722696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing Steps    \n\n   - __Feature Encoding__\n       + `Pclass` indicates the social/financial class/status of a person i.e., an ordinal relation between the categories hence I'm preserving the original data (numbers) hence they already have ordinal relation (1, 2, 3).\n       + For `Sex` and `Embarked`, there are two (similar) ways of encoding them.\n           * `pd.get_dummies(drop_first = True)`, which creates dummy variable following by dropping the firt category.\n           * `OneHotEncoder(sparse=False)`, which does a binary encoding and returns a dense matrix. (similar)\n   \n   \n   - __Handling Missing Values__\n       + Almost 75% entries under the `Cabin` feature are blank so I'm dropping it. (Should I handle it in a different way next time?)\n       + `Age` is fairly close to normal distribution and contains 18% of blank entries so It can be `imputed` by `mean` strategy.\n       + `Embarked` has only two null values and has some `class imbalance` as `S` is high in number.\n       + We can either \n           - [x] Drop those two records, or, \n           - [ ] Fill them with `mode` i.e., `S`      \n   \n   \n   - __Feature Elimination__\n       + Dropping `Name` and `PassengerId` as they do not seem to be of much relevance (read, 'Importance'). (Am I right about that?!)\n       + Dropping `Cabin`","metadata":{}},{"cell_type":"code","source":"# New set of categorical features\ncategorical_features_2 = ['Pclass', 'Sex', 'Embarked']","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:53.725492Z","iopub.execute_input":"2023-09-25T15:13:53.725858Z","iopub.status.idle":"2023-09-25T15:13:53.731741Z","shell.execute_reply.started":"2023-09-25T15:13:53.725826Z","shell.execute_reply":"2023-09-25T15:13:53.730306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, roc_curve, auc\n\n# Setting up transformers \nnumerical_transformer = Pipeline( steps= [\n    (\"imputer\", SimpleImputer(strategy='median')),\n    (\"scaler\", RobustScaler())\n])\n    \n\ncategorical_transformer = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n\n# Creating Preprocessor\npreprocessor = ColumnTransformer(transformers=[\n    ('numerical_transformer', numerical_transformer, numerical_features),\n    ('categorical_transformer', categorical_transformer, categorical_features_2),\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:53.733557Z","iopub.execute_input":"2023-09-25T15:13:53.73444Z","iopub.status.idle":"2023-09-25T15:13:54.10931Z","shell.execute_reply.started":"2023-09-25T15:13:53.734402Z","shell.execute_reply":"2023-09-25T15:13:54.108061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nrfc = RandomForestClassifier(n_jobs=-1, random_state=42)\nlr = LogisticRegression(random_state=42)\n\nbase_pipeline = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('lr', lr)\n])\n\n# rfc.get_params()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:54.110867Z","iopub.execute_input":"2023-09-25T15:13:54.111311Z","iopub.status.idle":"2023-09-25T15:13:54.141855Z","shell.execute_reply.started":"2023-09-25T15:13:54.111278Z","shell.execute_reply":"2023-09-25T15:13:54.1406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Selecting X & y\nX = titanic_df[numerical_features+categorical_features_2]\ny = titanic_df.Survived\n\n# Three set spliting \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n# Base\nbase_pipeline.fit(X_train, y_train)\nbase_pipeline.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:54.144372Z","iopub.execute_input":"2023-09-25T15:13:54.145552Z","iopub.status.idle":"2023-09-25T15:13:54.190117Z","shell.execute_reply.started":"2023-09-25T15:13:54.145503Z","shell.execute_reply":"2023-09-25T15:13:54.188935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_scores = cross_val_score(base_pipeline, X_train, y_train, cv=5)\ncv_scores.mean()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:54.191977Z","iopub.execute_input":"2023-09-25T15:13:54.192652Z","iopub.status.idle":"2023-09-25T15:13:54.353907Z","shell.execute_reply.started":"2023-09-25T15:13:54.192618Z","shell.execute_reply":"2023-09-25T15:13:54.353088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Stepping UP\n\n# Getting Data Ready","metadata":{}},{"cell_type":"code","source":"# Importing data\ntitanic_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# Shape and Features\nshape = titanic_df.shape\nfeatures = titanic_df.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:54.35503Z","iopub.execute_input":"2023-09-25T15:13:54.355805Z","iopub.status.idle":"2023-09-25T15:13:54.372947Z","shell.execute_reply.started":"2023-09-25T15:13:54.355772Z","shell.execute_reply":"2023-09-25T15:13:54.371839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Different Imputation Strategies\n\n\nThere are a number of different imputation strategies that can be used for the \"age\" variable in the Titanic dataset. Here are some of the most common strategies, along with their pros and cons:\n\n+ Mean imputation: This is the simplest imputation strategy, and it involves replacing all missing values with the mean age of all passengers. This strategy is easy to implement, but it can introduce bias into the data, as it assumes that all passengers have the same age distribution.\n+ Median imputation: This is similar to mean imputation, but it replaces missing values with the median age of all passengers. This strategy is less likely to introduce bias than mean imputation, as it is less sensitive to outliers.\n+ 3. Mode imputation: This strategy replaces missing values with the most frequent age in the dataset. This strategy is also easy to implement, but it can also introduce bias, as it assumes that all passengers are most likely to be the same age.\n+ 4. KNN imputation: This strategy uses k-nearest neighbors to impute missing values. KNN imputation is more complex than the other strategies, but it can also be more accurate, as it takes into account the relationships between different variables in the dataset.\n+ Bayesian imputation: This strategy uses Bayesian statistics to impute missing values. Bayesian imputation is more complex than KNN imputation, but it can be even more accurate, as it takes into account the uncertainty in the data.\n\n        The best imputation strategy for the \"age\" variable in the Titanic dataset will depend on a number of factors, including the size of the dataset, the distribution of the data, and the specific goals of the analysis. In general, mean imputation is a good starting point, but KNN or Bayesian imputation may be more appropriate for larger datasets or datasets with complex relationships between variables.\n\nHere is a table that summarizes the pros and cons of each imputation strategy:\n\n\n| Imputation Strategy | Pros | Cons |\n|---|---|---|\n| Mean imputation | Easy to implement | Can introduce bias |\n| Median imputation | Less likely to introduce bias than mean imputation | Can still introduce bias |\n| Mode imputation | Easy to implement | Can introduce bias |\n| KNN imputation | More accurate than mean, median, or mode imputation | More complex to implement |\n| Bayesian imputation | Most accurate imputation strategy | Most complex to implement |\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T08:38:37.137679Z","iopub.execute_input":"2023-08-14T08:38:37.138121Z","iopub.status.idle":"2023-08-14T08:38:37.142291Z","shell.execute_reply.started":"2023-08-14T08:38:37.138086Z","shell.execute_reply":"2023-08-14T08:38:37.141243Z"}}},{"cell_type":"code","source":"# Popular Imputers\nfrom sklearn.impute import SimpleImputer, KNNImputer\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\n# now you can import normally from sklearn.impute\nfrom sklearn.impute import IterativeImputer # BayesianRidge() is the default estimator\n\n\ndef impact_of_imputation(data, feature, constant=None):\n    \"\"\"\n    Plots the distribution of a feature before and after imputation with different strategies.\n\n    Args:\n        data (pd.DataFrame): The dataset to be analyzed.\n        feature (str): The name of the feature to be plotted.\n        constant (float): The constant value to be used for constant imputation.\n\n    Returns:\n        None.\n    \"\"\"\n\n    fig, axes = plt.subplots(3, 3, figsize = (16,16))\n    strategies = ['most_frequent', 'median', 'constant', 'mean', 'bayesian', 'knn']\n    \n    plt.suptitle(\"Different Imputation Strategies and Their affect on Numerical Data\")\n    \n    for index, strategy in enumerate(strategies):\n        temp_df = data.copy()\n        if strategy == 'constant':\n            imputer = SimpleImputer(strategy=strategy, fill_value=constant)\n        elif strategy == 'bayesian':\n            imputer = IterativeImputer()\n        elif strategy == 'knn':\n            imputer = KNNImputer(n_neighbors=5)\n        else:\n            imputer = SimpleImputer(strategy=strategy)\n        \n        sns.kdeplot(temp_df[feature], ax=axes[index // 3, index % 3])\n        temp_df[numerical_features] = imputer.fit_transform(temp_df[numerical_features])\n        sns.kdeplot(temp_df[feature], ax=axes[index // 3, index % 3]).set_title(\n            f\"{feature} after {strategy} Imputation\"\n        )\n    \n    for i in range(3):\n        axes[2, i].remove()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:54.374228Z","iopub.execute_input":"2023-09-25T15:13:54.375444Z","iopub.status.idle":"2023-09-25T15:13:54.389666Z","shell.execute_reply.started":"2023-09-25T15:13:54.375364Z","shell.execute_reply":"2023-09-25T15:13:54.388698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impact_of_imputation(titanic_df, \"Age\")","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:54.391201Z","iopub.execute_input":"2023-09-25T15:13:54.391603Z","iopub.status.idle":"2023-09-25T15:13:57.093762Z","shell.execute_reply.started":"2023-09-25T15:13:54.39156Z","shell.execute_reply":"2023-09-25T15:13:57.092557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Different Standardization Methods\n\nThere are several standardization methods for the job.\n\n+ Min-max normalization: This method involves subtracting the minimum value of each feature from each value of that feature, and then dividing by the difference between the maximum and minimum values of that feature. This results in all features having a range of 0 to 1.\n    + Pros: Min-max normalization is also easy to implement and understand. It is not as sensitive to outliers as z-score normalization, making it a good choice for data with outliers.\n    - Cons: Min-max normalization can shrink the range of some features, which can make it difficult to see the differences between them.\n\n\n+ StandardScaler: This is a more sophisticated method of standardization that is implemented in the Scikit-learn library. It uses a technique called robust scaling to minimize the impact of outliers.\n    + Pros: StandardScaler is more robust to outliers than z-score normalization or min-max normalization. It is also a good choice for data with a mix of continuous and categorical features.\n    - Cons: StandardScaler is more complex to implement than z-score normalization or min-max normalization.\n\n\n+ Robust scaler: This method is similar to z-score normalization, but it is more robust to outliers. It works by subtracting the median of each feature from each value of that feature, and then dividing by the interquartile range (IQR) of that feature. This results in all features having a mean of 0 and a standard deviation of 1, but it is less affected by outliers.\n    + Pros: Robust scaler is more robust to outliers than z-score normalization. It is also a good choice for data with a mix of continuous and categorical features.\n    - Cons: Robust scaler is more complex to implement than z-score normalization.\n    \n    \n| Method | Pros | Cons |\n|---|---|---|\n| Robust scaler | More robust to outliers than z-score normalization. Good choice for data with a mix of continuous and categorical features. | More complex to implement than z-score normalization. |\n| Min-max normalization | Not as sensitive to outliers as z-score normalization. Easy to implement and understand. | Can shrink the range of some features. |\n| StandardScaler | Similar to z-score normalization, but more robust to outliers. | More complex to implement than z-score normalization or min-max normalization. |\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n\ndef scale_and_plot(data, scalers, numerical_features):\n    \"\"\"Scales the data using the specified scalers and plots the distribution of the data after scaling.\n\n    Args:\n        data: The data to scale.\n        scalers: A dictionary of scalers to use.\n        numerical_features: The numerical features to scale.\n    \"\"\"\n\n    fig, ax = plt.subplots(4, 2, figsize=(16, 16))\n\n    for scaler_name, scaler in scalers.items():\n        df = data.copy()\n        scaled_data = scaler.fit_transform(df[numerical_features])\n        for index, feature in enumerate(numerical_features):\n            sns.kdeplot(titanic_df[numerical_features[index]], ax=ax[index, 0])\n#             sns.kdeplot(np.log1p(titanic_df[numerical_features[index]]), ax=ax[index, 0])\n            sns.kdeplot(scaled_data[:, index], ax=ax[index, 1]).set_title(f\"{feature} - {scaler_name}\")\n            ax[index, 1].legend(list(scalers.keys()))\n    \n    plt.show()\n\nscale_and_plot(titanic_df, {\n        'Min-Max Scaler': MinMaxScaler(),\n        'Standard Scaler': StandardScaler(),\n        'Robust Scaler': RobustScaler()\n    }, numerical_features = numerical_features)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:13:57.095567Z","iopub.execute_input":"2023-09-25T15:13:57.095939Z","iopub.status.idle":"2023-09-25T15:14:00.027922Z","shell.execute_reply.started":"2023-09-25T15:13:57.095905Z","shell.execute_reply":"2023-09-25T15:14:00.026686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Transformation Strategies","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\nyj_transformer = PowerTransformer()\n\ntransformed_yj = yj_transformer.fit_transform(titanic_df[numerical_features])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:14:00.029553Z","iopub.execute_input":"2023-09-25T15:14:00.029944Z","iopub.status.idle":"2023-09-25T15:14:00.058148Z","shell.execute_reply.started":"2023-09-25T15:14:00.029907Z","shell.execute_reply":"2023-09-25T15:14:00.056931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.errstate(invalid='ignore')\n\n# Create a figure with four subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))#, subplot_kw={'title': 'Distribution of {} values'.format(numf)})\n\n# Plot the distributions of the original data, log-transformed data, and transformed data\nfor index, numf in enumerate(numerical_features):\n    sns.kdeplot(titanic_df[numf], ax=axes[index // 2, index % 2],color='red')\n    sns.kdeplot(np.log1p(titanic_df[numf]), ax=axes[index // 2, index % 2],color='orange')\n    sns.kdeplot(np.sqrt(titanic_df[numf]), ax=axes[index // 2, index % 2],color='purple')\n    sns.kdeplot(transformed_yj[index], ax=axes[index // 2, index % 2],color='green')\n#     axes[index // 2, index % 2].set_title(numf)\n\n# Create a custom legend\nlegend_elements = [\n    plt.Line2D([0], [0], color='red', label='Original data'),\n    plt.Line2D([0], [0], color='orange', label='Log-transformed data'),\n    plt.Line2D([0], [0], color='purple', label='Square root transformed data'),\n    plt.Line2D([0], [0], color='green', label='YJ transformed data')\n]\n\n# Add the custom legend to the figure\nfig.legend(handles=legend_elements, title='Transformations', loc='upper right', prop={'size': 12})\n\n# Tighten the layout of the subplots\nplt.tight_layout()\n\n# Show the figure\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:15:03.08976Z","iopub.execute_input":"2023-09-25T15:15:03.09017Z","iopub.status.idle":"2023-09-25T15:15:04.758874Z","shell.execute_reply.started":"2023-09-25T15:15:03.090138Z","shell.execute_reply":"2023-09-25T15:15:04.757653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying different models on our dataset.","metadata":{}},{"cell_type":"code","source":"# Drop the Cabin and Name features\n# titanic_df = titanic_df.drop([\"Cabin\", \"Name\"], axis=1)\n\n# Create the X and y\n\n## One-hot encode the categorical features\nonehot_encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\nX_cat = onehot_encoder.fit_transform(titanic_df[categorical_features_2])\n\n## Impute the missing values\nbayesian_imputer = IterativeImputer()\nX_num = bayesian_imputer.fit_transform(titanic_df[numerical_features])\n\n## Scale the features\nrobust_scaler = RobustScaler()\nX = robust_scaler.fit_transform(np.concatenate((X_cat, X_num), axis=1))\ny = titanic_df[\"Survived\"]\n\n## Transform the values\nyj_transformer = PowerTransformer()\nX = yj_transformer.fit_transform(X_num)\n\n## Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:15:09.280081Z","iopub.execute_input":"2023-09-25T15:15:09.280591Z","iopub.status.idle":"2023-09-25T15:15:09.333944Z","shell.execute_reply.started":"2023-09-25T15:15:09.280554Z","shell.execute_reply":"2023-09-25T15:15:09.333048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing models\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\n\n# creating a model dictionary\nmodels = {\n    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n    'LinearSVC': LinearSVC(random_state=42, max_iter=100000),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n    \"XGBoost\": XGBClassifier(random_state=42),\n    \"Light GBM\": LGBMClassifier(random_state=42),\n}\n\n# Training \nfor model_name, model in models.items():\n    model.fit(X_train, y_train)\n\n# Evaluate the models\nscores = {}\nfor model_name, model in models.items():\n    y_pred = model.predict(X_test)\n    scores[model_name] = accuracy_score(y_test, y_pred)\n\n# Sorting by score\nscores = dict(sorted(scores.items(), \n                key=lambda x:x[1], reverse=True))\n# Print the scores\n\nprint(\"Model scores:\")\nfor model_name, score in scores.items():\n    print(f\"{model_name}: {score}\")\n\n\n# Plotting\nsns.barplot(pd.DataFrame(scores, index=[\"Accuracy\"]), \n            orient = 'h', color = \"Lightblue\").set_title(\"Comparison of Model Scores\");","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:15:09.620946Z","iopub.execute_input":"2023-09-25T15:15:09.622001Z","iopub.status.idle":"2023-09-25T15:15:11.868134Z","shell.execute_reply.started":"2023-09-25T15:15:09.62196Z","shell.execute_reply":"2023-09-25T15:15:11.867282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimizing Further","metadata":{}},{"cell_type":"code","source":"# utility code for dropping columns\nclass DropColumns(object):\n    def __init__(self, drop_columns):\n        self.drop_columns = drop_columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.drop(self.drop_columns, axis=1)\n        return X","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:15:11.869793Z","iopub.execute_input":"2023-09-25T15:15:11.870792Z","iopub.status.idle":"2023-09-25T15:15:11.877326Z","shell.execute_reply.started":"2023-09-25T15:15:11.870745Z","shell.execute_reply":"2023-09-25T15:15:11.876234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting X & y\nX = titanic_df[numerical_features+categorical_features_2]\ny = titanic_df.Survived\n\n# Three set spliting \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n# Drop the Cabin and Name features\n# drop_columns = [\"Cabin\", \"Name\"]\n\n# Setting up transformers \nnumerical_transformer_2 = Pipeline( steps= [\n    (\"bayesian_imputer\", bayesian_imputer),\n    (\"robust_scaler\", RobustScaler()),\n    (\"yj_transformer\", PowerTransformer())\n])\n    \n# Categorical transformer\ncategorical_transformer_2 = Pipeline(steps= [\n#     (\"drop\", DropColumns(drop_columns)),\n    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n                                          ])\n\n# Creating Preprocessor\npreprocessor_2 = ColumnTransformer(transformers=[\n    ('numerical_transformer', numerical_transformer_2, numerical_features),\n    ('categorical_transformer', categorical_transformer_2, categorical_features_2),\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:15:12.855862Z","iopub.execute_input":"2023-09-25T15:15:12.856257Z","iopub.status.idle":"2023-09-25T15:15:12.868109Z","shell.execute_reply.started":"2023-09-25T15:15:12.856224Z","shell.execute_reply":"2023-09-25T15:15:12.866905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the pipeline\npipeline = Pipeline(steps=[\n    (\"preprocessor_2\", preprocessor_2), \n    (\"gradboost\", GradientBoostingClassifier(random_state = 42))\n])\n\n# Fit the model\n# pipeline.fit(titanic_df.drop(\"Survived\",axis = 1), titanic_df[\"Survived\"])\npipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:15:21.418291Z","iopub.execute_input":"2023-09-25T15:15:21.418768Z","iopub.status.idle":"2023-09-25T15:15:21.742762Z","shell.execute_reply.started":"2023-09-25T15:15:21.418731Z","shell.execute_reply":"2023-09-25T15:15:21.741606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pipeline.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:15:21.745003Z","iopub.execute_input":"2023-09-25T15:15:21.745563Z","iopub.status.idle":"2023-09-25T15:15:21.763624Z","shell.execute_reply.started":"2023-09-25T15:15:21.745518Z","shell.execute_reply":"2023-09-25T15:15:21.762471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline[\"gradboost\"].get_params()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:15:26.516057Z","iopub.execute_input":"2023-09-25T15:15:26.516554Z","iopub.status.idle":"2023-09-25T15:15:26.531549Z","shell.execute_reply.started":"2023-09-25T15:15:26.516501Z","shell.execute_reply":"2023-09-25T15:15:26.530225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    \"gradboost__n_estimators\": [60, 70, 80 ,90],\n    \"gradboost__learning_rate\": [0.1, 0.3, 0.5, 0.7],\n#     \"gradboost__max_depth\": [3, 5, 10],\n#     \"gradboost__min_samples_split\": [2, 5, 10],\n#     \"gradboost__min_samples_leaf\": [1, 2, 5],\n}\n\ngrid_search = GridSearchCV(pipeline, params, \n                           cv=5,\n                          verbose=2)\ngrid_search.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:19:50.824765Z","iopub.execute_input":"2023-09-25T15:19:50.825173Z","iopub.status.idle":"2023-09-25T15:20:04.90229Z","shell.execute_reply.started":"2023-09-25T15:19:50.825139Z","shell.execute_reply":"2023-09-25T15:20:04.901208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:20:04.904034Z","iopub.execute_input":"2023-09-25T15:20:04.904361Z","iopub.status.idle":"2023-09-25T15:20:04.912852Z","shell.execute_reply.started":"2023-09-25T15:20:04.904332Z","shell.execute_reply":"2023-09-25T15:20:04.911302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:20:04.933444Z","iopub.execute_input":"2023-09-25T15:20:04.933855Z","iopub.status.idle":"2023-09-25T15:20:04.941677Z","shell.execute_reply.started":"2023-09-25T15:20:04.933822Z","shell.execute_reply":"2023-09-25T15:20:04.940365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # rfc_optimised = RandomForestClassifier(\n# #     n_estimators = 1900,\n# #     min_samples_split = 2,\n# #     min_samples_leaf = 6,\n# #     max_leaf_nodes = 8,\n# #     criterion = 'entropy',\n# #     n_jobs = -1)\n\n# # final_pipeline = Pipeline(steps = [\n# #     ('preprocessor', preprocessor),\n# #     ('rfc_optimised', rfc_optimised),\n# # ])\n\n# # final_pipeline.fit(X, y)\n\npipeline = Pipeline(steps=[\n    (\"preprocessor_2\", preprocessor_2), \n    (\"gradboost\", GradientBoostingClassifier(random_state = 42,\n                                             n_estimators=70, \n                                             learning_rate=0.5))\n])\n\npipeline.fit(X,y)\n\npredictions = pipeline.predict(test_df)\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:25:04.477753Z","iopub.execute_input":"2023-09-25T15:25:04.478159Z","iopub.status.idle":"2023-09-25T15:25:04.667387Z","shell.execute_reply.started":"2023-09-25T15:25:04.478128Z","shell.execute_reply":"2023-09-25T15:25:04.666147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-14T07:26:08.927925Z","iopub.execute_input":"2023-08-14T07:26:08.92844Z","iopub.status.idle":"2023-08-14T07:26:08.939242Z","shell.execute_reply.started":"2023-08-14T07:26:08.9284Z","shell.execute_reply":"2023-08-14T07:26:08.936951Z"}}},{"cell_type":"markdown","source":"## References\n1. [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic/overview)\n2. [Titanic Tutorial](https://www.kaggle.com/code/alexisbcook/titanic-tutorial/notebook) by [Alexis Cook](https://www.kaggle.com/alexisbcook)\n3. [Complete Machine Learning and Data Science Bootcamp](https://zerotomastery.io/courses/machine-learning-and-data-science-bootcamp/) by [ZTM](https://zerotomastery.io/)\n4. [Iterative Imputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)","metadata":{}}]}